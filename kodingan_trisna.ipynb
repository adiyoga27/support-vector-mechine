{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANALISIS SENTIMEN TERHADAP KEBIJAKAN FULL DAY SCHOOL DENGAN METODE SUPPORT VECTOR MACHINE (SVM)\n",
        "Aplikasi ini menggunakan methode SVM. ada pun library yang harus digunakan antara lainnya :  \n",
        "1. Install pandas: pip install pandas\n",
        "2. Install numpy: pip install numpy\n",
        "3. Install matplotlib: pip install matplotlib\n",
        "4. Install openpyxl: pip install openpyxl\n",
        "5. Install string: pip install string\n",
        "6. Install re: pip install re\n",
        "7. Install nltk: pip install nltk\n",
        "8. Install swifter: pip install swifter\n",
        "9. Install deep_translator: pip install deep-translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NAiN_lrwlqM3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import openpyxl\n",
        "import string\n",
        "import re #regex library\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import swifter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - PERISAPAAN DATA LATIH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "Qqcixw4WtZz1",
        "outputId": "f871b7d4-070a-4efa-df51-e35673614797"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Udah 2022, SMA saya juga ga full day, dan tete...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Halo Haikal, saya dari masa depan tepatnya tah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gak bisa nonton anime eek</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;a href=\"http://www.youtube.com/results?search...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10123</th>\n",
              "      <td>Gua kelas 9 (3 smp) menurut gua sih sekolah fu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10124</th>\n",
              "      <td>kalau full day nya versi kaya ko   sih saya se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10125</th>\n",
              "      <td>bisa sih full day asal banyaknya istirahat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10126</th>\n",
              "      <td>kalaw saya sekolah agam islam jadi apa termasu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10127</th>\n",
              "      <td>Gak lah bang capek lah entar sekolahnya belum ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10128 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet\n",
              "0      Udah 2022, SMA saya juga ga full day, dan tete...\n",
              "1      Saya sampai demam karena lamanya sekolah dan p...\n",
              "2      Halo Haikal, saya dari masa depan tepatnya tah...\n",
              "3                              Gak bisa nonton anime eek\n",
              "4      <a href=\"http://www.youtube.com/results?search...\n",
              "...                                                  ...\n",
              "10123  Gua kelas 9 (3 smp) menurut gua sih sekolah fu...\n",
              "10124  kalau full day nya versi kaya ko   sih saya se...\n",
              "10125         bisa sih full day asal banyaknya istirahat\n",
              "10126  kalaw saya sekolah agam islam jadi apa termasu...\n",
              "10127  Gak lah bang capek lah entar sekolahnya belum ...\n",
              "\n",
              "[10128 rows x 1 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_excel('assets/data_latih.xlsx')\n",
        "df=pd.DataFrame(data[['tweet']])\n",
        "\n",
        "df.to_excel(\"output/processing/tweet.xlsx\", index=False)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - PROCESSING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "I9lZLNQcuY31",
        "outputId": "4281ec8b-676b-4733-f32a-bfcbf1b4498d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mw/zf7hzj1d5nj9129k4nc_8cpm0000gp/T/ipykernel_8939/190367052.py:49: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>case folding</th>\n",
              "      <th>data cleansing</th>\n",
              "      <th>tokenization</th>\n",
              "      <th>normalized_term</th>\n",
              "      <th>stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Udah 2022, SMA saya juga ga full day, dan tete...</td>\n",
              "      <td>udah 2022, sma saya juga ga full day, dan tete...</td>\n",
              "      <td>udah sma saya juga ga full day dan tetep aja g...</td>\n",
              "      <td>[udah, sma, saya, juga, ga, full, day, dan, te...</td>\n",
              "      <td>[sudah, sama, saya, juga, tidak, full, day, da...</td>\n",
              "      <td>[aja, kemajuan, aduh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>[saya, sampai, demam, karena, lamanya, sekolah...</td>\n",
              "      <td>[saya, sampai, demam, karena, lama, sekolah, d...</td>\n",
              "      <td>[demam, sekolah, pelajaran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Halo Haikal, saya dari masa depan tepatnya tah...</td>\n",
              "      <td>halo haikal, saya dari masa depan tepatnya tah...</td>\n",
              "      <td>halo haikal saya dari masa depan tepatnya tahu...</td>\n",
              "      <td>[halo, haikal, saya, dari, masa, depan, tepatn...</td>\n",
              "      <td>[halo, haikal, saya, dari, masa, depan, tepatn...</td>\n",
              "      <td>[halo, tepatnya, sekolah, rumah, wabah, virus,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gak bisa nonton anime eek</td>\n",
              "      <td>gak bisa nonton anime eek</td>\n",
              "      <td>gak bisa nonton anime eek</td>\n",
              "      <td>[gak, bisa, nonton, anime, eek]</td>\n",
              "      <td>[tidak, bisa, nonton, anime, tai]</td>\n",
              "      <td>[nonton, anime, tai]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;a href=\"http://www.youtube.com/results?search...</td>\n",
              "      <td>&lt;a href=\"http://www.youtube.com/results?search...</td>\n",
              "      <td>yang aku suka adalah ko contohin untuk nyari i...</td>\n",
              "      <td>[yang, aku, suka, adalah, ko, contohin, untuk,...</td>\n",
              "      <td>[yang, aku, suka, adalah, kok, contoh, untuk, ...</td>\n",
              "      <td>[suka, contoh, mencari, info, komen]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  \\\n",
              "0  Udah 2022, SMA saya juga ga full day, dan tete...   \n",
              "1  Saya sampai demam karena lamanya sekolah dan p...   \n",
              "2  Halo Haikal, saya dari masa depan tepatnya tah...   \n",
              "3                          Gak bisa nonton anime eek   \n",
              "4  <a href=\"http://www.youtube.com/results?search...   \n",
              "\n",
              "                                        case folding  \\\n",
              "0  udah 2022, sma saya juga ga full day, dan tete...   \n",
              "1  saya sampai demam karena lamanya sekolah dan p...   \n",
              "2  halo haikal, saya dari masa depan tepatnya tah...   \n",
              "3                          gak bisa nonton anime eek   \n",
              "4  <a href=\"http://www.youtube.com/results?search...   \n",
              "\n",
              "                                      data cleansing  \\\n",
              "0  udah sma saya juga ga full day dan tetep aja g...   \n",
              "1  saya sampai demam karena lamanya sekolah dan p...   \n",
              "2  halo haikal saya dari masa depan tepatnya tahu...   \n",
              "3                          gak bisa nonton anime eek   \n",
              "4  yang aku suka adalah ko contohin untuk nyari i...   \n",
              "\n",
              "                                        tokenization  \\\n",
              "0  [udah, sma, saya, juga, ga, full, day, dan, te...   \n",
              "1  [saya, sampai, demam, karena, lamanya, sekolah...   \n",
              "2  [halo, haikal, saya, dari, masa, depan, tepatn...   \n",
              "3                    [gak, bisa, nonton, anime, eek]   \n",
              "4  [yang, aku, suka, adalah, ko, contohin, untuk,...   \n",
              "\n",
              "                                     normalized_term  \\\n",
              "0  [sudah, sama, saya, juga, tidak, full, day, da...   \n",
              "1  [saya, sampai, demam, karena, lama, sekolah, d...   \n",
              "2  [halo, haikal, saya, dari, masa, depan, tepatn...   \n",
              "3                  [tidak, bisa, nonton, anime, tai]   \n",
              "4  [yang, aku, suka, adalah, kok, contoh, untuk, ...   \n",
              "\n",
              "                                           stopwords  \n",
              "0                              [aja, kemajuan, aduh]  \n",
              "1                        [demam, sekolah, pelajaran]  \n",
              "2  [halo, tepatnya, sekolah, rumah, wabah, virus,...  \n",
              "3                               [nonton, anime, tai]  \n",
              "4               [suka, contoh, mencari, info, komen]  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Fungsi untuk memeriksa dan mengunduh NLTK data jika belum ada\n",
        "def check_nltk_data():\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "# Memeriksa dan mengunduh data yang dibutuhkan\n",
        "check_nltk_data()\n",
        "\n",
        "# Baca data dari Excel\n",
        "df = pd.read_excel('output/processing/tweet.xlsx')\n",
        "\n",
        "# ======================== PROSES UNTUK CLEANING DATA ============================\n",
        "\n",
        "def remove_tweet_special(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Jika bukan string, kembalikan data kosong\n",
        "    \n",
        "    # Hilangkan tab, baris baru, dan karakter backslash\n",
        "    text = text.replace('\\t', \" \").replace('\\n', \" \").replace('\\\\', \" \").replace(\"-\", \" \").replace(\",\", \" \").replace(\".\", \" \")\n",
        "    \n",
        "    # Hilangkan karakter non-ASCII (emotikon, karakter Cina, dll.)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    \n",
        "    # Hilangkan mention, link, dan hashtag\n",
        "    text = re.sub(r'[@#][A-Za-z0-9]+|https?://\\S+', \" \", text)\n",
        "    \n",
        "    # Hilangkan spasi berlebih\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    text = text.replace(\"<br>\", \" \").replace(\"<br/>\", \" \").replace(\"<p>\", \" \").replace(\"</p>\", \" \").replace(\"<h1>\", \" \").replace(\"</h1>\", \" \").replace(\"<h2>\",\" \").replace(\"<h3>\",\" \").replace(\"<h4>\",\" \").replace(\"href\", \" \")\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text()\n",
        "    clean_text = ' '.join(clean_text.split())\n",
        "    return clean_text\n",
        "\n",
        "def remove_number(text):\n",
        "    return re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "def remove_single_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "# Proses case folding\n",
        "df['case folding'] = df['tweet'].str.lower()\n",
        "\n",
        "# Terapkan fungsi pembersihan\n",
        "df['data cleansing'] = df['case folding'].apply(remove_tweet_special)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_html_tags)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_number)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_punctuation)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_whitespace_LT)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_whitespace_multiple)\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_single_char)\n",
        "\n",
        "df.to_excel(\"output/processing/cleansing.xlsx\", index=False)\n",
        "\n",
        "# ======================== PROSES UNTUK TOKENIZATION DATA ============================\n",
        "\n",
        "def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['tokenization'] = df['data cleansing'].apply(tokenization)\n",
        "df.to_excel(\"output/processing/tokenization.xlsx\", index=False)\n",
        "\n",
        "# ======================== PROSES UNTUK NORMALISASI DATA ============================\n",
        "\n",
        "normalized_word = pd.read_excel(\"assets/normalisasi.xlsx\")\n",
        "\n",
        "# Membuat kamus untuk normalisasi\n",
        "normalized_word_dict = {}\n",
        "for index, row in normalized_word.iterrows():\n",
        "    normalized_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalized_word_dict.get(term, term) for term in document]\n",
        "\n",
        "df['normalized_term'] = df['tokenization'].apply(normalized_term)\n",
        "df.to_excel(\"output/processing/normalized_term.xlsx\", index=False)\n",
        "\n",
        "# ======================== PROSES UNTUK STOPWORD DATA ============================\n",
        "\n",
        "# Menggunakan Indonesian stopwords\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# Menambahkan stopword yang dibutuhkan ke list stopwords\n",
        "list_stopwords.extend([\n",
        "    'full', 'day', 'nik', 'ais', 'ih', 'kuea', 'ndes', 'tk', 'arg', 'hhhh', 'wuakakak',\n",
        "    'gtth', 'wowww', 'apeeee', 'Aksjsjsk', 'alaee', 'koq', 'salengpraew', 'rukkhadevata', \n",
        "    'zeon', 'vivienne', 'yaam', 'woyy', 'ykwi', 'auff', 'ue', 'hoek', 'hayo', 'chnmn', \n",
        "    'hahahah', 'haaaaaa', 'din', 'woy', 'ndeer', 'lalalala', 'wkwkwkwkwkkw', 'woyyy', \n",
        "    'dih', 'den', 'hehew', 'etdah', 'beeeuh', 'wahh', 'heheee', 'hhaaha', 'waaaaa', \n",
        "    'oakilah', 'haaaahh', 'huft', 'ai', 'et', 'acha', 'hokyahokya', 'hahahihi', 'yl', \n",
        "    'wihh', 'hahahaa', 'hhhh', 'def', 'ayom', 'ser', 'duh', 'heuheueheu', 'huwaaaaaa', \n",
        "    'yalah', 'mww', 'cekabia', 'dikatara', 'angganara', 'krtsk', 'woee', 'ndi', 'ohh', \n",
        "    'www', 'aee', 'huaaaa', 'gn', 'hahahah', 'nd', 'ema', 'ceratops', 'pasuk', 'ygy', \n",
        "    'repp', 'gais', 'hadehhh', 'walah', 'hahah', 'paa', 'awkwkwk', 'wkwkk', 'wkwkw', \n",
        "    'wkwkwkwkwkwah', 'haikal', 'wkwkwkw', 'baceprot', 'sksksk', 'heheh', 'brooo', \n",
        "    'dbd', 'aeee', 'weeeh', 'wehh', 'milta', 'hsnah', 'swsg', 'hemm', 'xda', 'yara', \n",
        "    'ohh', 'heh', 'kle', 'acy', 'hayooo', 'hahahahaha', 'balablablabla', 'lai', 'loj', \n",
        "    'itine', 'heehehe', 'kwkwk', 'kwkwkwkwwkwk', 'waaa', 'demending', 'pali', 'eeh', \n",
        "    'dlsb', 'cooooy', 'hehehehe', 'adjem', 'aih', 'syar', 'wkwkk', 'aowkwkwk', 'walah', \n",
        "    'euy', 'der', 'hahaa', 'hesteg', 'hmmmmtar', 'gtideologi', 'ab', 'owkwkwkwk', 'dncw', \n",
        "    'sloga', 'jo', 'jengjenggg', 'anuanu', 'caw', 'ehehheheh', 'hlaa', 'hahahihi', \n",
        "    'ckckckck', 'sich', 'pakin', 'mmarkpkk', 'ponponpon', 'kyary', 'pamyu', 'laaahhh', \n",
        "    'cp', 'duhhh', 'eno', 'lise', 'bi', 'ieu', 'poho', 'boga', 'imah', 'keur', 'ulin', \n",
        "    'kwkwkw', 'ehheh', 'gryli', 'oalah', 'prekk', 'hehh', 'cere', 'ekekekek', 'chco', \n",
        "    'nganu', 'wkwkkwkwkwkwkw', 'zell', 'awowkwkwkwk', 'kinyis', 'pus', 'yng', 'yg', \n",
        "    'yang', 'wkwoswkwo', 'wkwkwkwkwkwk', 'ahahha', 'weeeeh', 'hah', 'nuuuuuuuuuuuuuuuuuuuuuuuuuuuuu', \n",
        "    'hong', 'jay', 'haikyuu', 'nderrr', 'omtanteuwaksodara', 'ahsajkakaka', 'kwkwkwk', \n",
        "    'derrr', 'wwkwkwkw', 'hadehh', 'aaaaa', 'heeh', 'dem', 'ocaaa', 'wo', 'prenup', \n",
        "    'dihhh', 'cokk', 'imho', 'chenle', 'jsdieksisnisawikwok', 'hahahahahahaha', 'bam', \n",
        "    'yowohh', 'lau', 'boiiiii', 'gih', 'beuhhh', 'wkw', 'wkwkwkw', 'dooong', 'oalaaaa', \n",
        "    'sinoeng', 'wkekwk', 'nyai', 'cai', 'anw', 'tjuyyy', 'hanss', 'mh', 'ih', 'widihh', \n",
        "    'cy', 'eeeee', 'gi', 'luat', 'laaaaa', 'cam', 'lancau', 'tuch', 'kun', 'uhhhh', \n",
        "    'chuakssss', 'oiyaa', 'hadeuhhhh', 'wkwkwkwwk', 'hehehee', 'nk', 'sih','nih', 'lak', \n",
        "    'qwq', 'oneesan', 'eeehmmm', 'am', 'wkwk', 'hahaha','zellnya', 'ea' ,'ealah','quot'\n",
        "])\n",
        "\n",
        "# Membaca stopwords tambahan dari file\n",
        "txt_stopword = pd.read_csv(\"assets/stopwordbahasa.txt\", names=[\"stopwords\"], header=None)\n",
        "list_stopwords.extend(txt_stopword[\"stopwords\"])\n",
        "\n",
        "# Konversi stopwords ke set\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['stopwords'] = df['normalized_term'].apply(stopwords_removal)\n",
        "df.to_excel(\"output/processing/stopwords.xlsx\", index=False)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - STEMMING MENGGUNAKAN LIBRARY SASTRAWI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Pandas Apply: 100%|██████████| 10128/10128 [00:00<00:00, 1090318.80it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>case folding</th>\n",
              "      <th>data cleansing</th>\n",
              "      <th>tokenization</th>\n",
              "      <th>normalized_term</th>\n",
              "      <th>stopwords</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Udah 2022, SMA saya juga ga full day, dan tete...</td>\n",
              "      <td>udah 2022, sma saya juga ga full day, dan tete...</td>\n",
              "      <td>udah sma saya juga ga full day dan tetep aja g...</td>\n",
              "      <td>[udah, sma, saya, juga, ga, full, day, dan, te...</td>\n",
              "      <td>[sudah, sama, saya, juga, tidak, full, day, da...</td>\n",
              "      <td>[aja, kemajuan, aduh]</td>\n",
              "      <td>[aja, maju, aduh]</td>\n",
              "      <td>aja maju aduh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>saya sampai demam karena lamanya sekolah dan p...</td>\n",
              "      <td>[saya, sampai, demam, karena, lamanya, sekolah...</td>\n",
              "      <td>[saya, sampai, demam, karena, lama, sekolah, d...</td>\n",
              "      <td>[demam, sekolah, pelajaran]</td>\n",
              "      <td>[demam, sekolah, ajar]</td>\n",
              "      <td>demam sekolah ajar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Halo Haikal, saya dari masa depan tepatnya tah...</td>\n",
              "      <td>halo haikal, saya dari masa depan tepatnya tah...</td>\n",
              "      <td>halo haikal saya dari masa depan tepatnya tahu...</td>\n",
              "      <td>[halo, haikal, saya, dari, masa, depan, tepatn...</td>\n",
              "      <td>[halo, haikal, saya, dari, masa, depan, tepatn...</td>\n",
              "      <td>[halo, tepatnya, sekolah, rumah, wabah, virus,...</td>\n",
              "      <td>[halo, tepat, sekolah, rumah, wabah, virus, tu...</td>\n",
              "      <td>halo tepat sekolah rumah wabah virus tular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gak bisa nonton anime eek</td>\n",
              "      <td>gak bisa nonton anime eek</td>\n",
              "      <td>gak bisa nonton anime eek</td>\n",
              "      <td>[gak, bisa, nonton, anime, eek]</td>\n",
              "      <td>[tidak, bisa, nonton, anime, tai]</td>\n",
              "      <td>[nonton, anime, tai]</td>\n",
              "      <td>[nonton, anime, tai]</td>\n",
              "      <td>nonton anime tai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;a href=\"http://www.youtube.com/results?search...</td>\n",
              "      <td>&lt;a href=\"http://www.youtube.com/results?search...</td>\n",
              "      <td>yang aku suka adalah ko contohin untuk nyari i...</td>\n",
              "      <td>[yang, aku, suka, adalah, ko, contohin, untuk,...</td>\n",
              "      <td>[yang, aku, suka, adalah, kok, contoh, untuk, ...</td>\n",
              "      <td>[suka, contoh, mencari, info, komen]</td>\n",
              "      <td>[suka, contoh, cari, info, komen]</td>\n",
              "      <td>suka contoh cari info komen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10123</th>\n",
              "      <td>Gua kelas 9 (3 smp) menurut gua sih sekolah fu...</td>\n",
              "      <td>gua kelas 9 (3 smp) menurut gua sih sekolah fu...</td>\n",
              "      <td>gua kelas smp menurut gua sih sekolah full day...</td>\n",
              "      <td>[gua, kelas, smp, menurut, gua, sih, sekolah, ...</td>\n",
              "      <td>[aku, kelas, sampai, menurut, aku, sih, sekola...</td>\n",
              "      <td>[kelas, sekolah]</td>\n",
              "      <td>[kelas, sekolah]</td>\n",
              "      <td>kelas sekolah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10124</th>\n",
              "      <td>kalau full day nya versi kaya ko   sih saya se...</td>\n",
              "      <td>kalau full day nya versi kaya ko   sih saya se...</td>\n",
              "      <td>kalau full day nya versi kaya ko sih saya setu...</td>\n",
              "      <td>[kalau, full, day, nya, versi, kaya, ko, sih, ...</td>\n",
              "      <td>[kalau, full, day, nya, versi, kaya, kok, sih,...</td>\n",
              "      <td>[nya, versi, kaya, setuju, banget, sekolah, pe...</td>\n",
              "      <td>[nya, versi, kaya, setuju, banget, sekolah, di...</td>\n",
              "      <td>nya versi kaya setuju banget sekolah didik mur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10125</th>\n",
              "      <td>bisa sih full day asal banyaknya istirahat</td>\n",
              "      <td>bisa sih full day asal banyaknya istirahat</td>\n",
              "      <td>bisa sih full day asal banyaknya istirahat</td>\n",
              "      <td>[bisa, sih, full, day, asal, banyaknya, istira...</td>\n",
              "      <td>[bisa, sih, full, day, asal, banyaknya, istira...</td>\n",
              "      <td>[banyaknya, istirahat]</td>\n",
              "      <td>[banyak, istirahat]</td>\n",
              "      <td>banyak istirahat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10126</th>\n",
              "      <td>kalaw saya sekolah agam islam jadi apa termasu...</td>\n",
              "      <td>kalaw saya sekolah agam islam jadi apa termasu...</td>\n",
              "      <td>kalaw saya sekolah agam islam jadi apa termasu...</td>\n",
              "      <td>[kalaw, saya, sekolah, agam, islam, jadi, apa,...</td>\n",
              "      <td>[kalaw, saya, sekolah, agama, islam, jadi, apa...</td>\n",
              "      <td>[kalaw, sekolah, agama, islam]</td>\n",
              "      <td>[kalaw, sekolah, agama, islam]</td>\n",
              "      <td>kalaw sekolah agama islam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10127</th>\n",
              "      <td>Gak lah bang capek lah entar sekolahnya belum ...</td>\n",
              "      <td>gak lah bang capek lah entar sekolahnya belum ...</td>\n",
              "      <td>gak lah bang capek lah entar sekolahnya belum ...</td>\n",
              "      <td>[gak, lah, bang, capek, lah, entar, sekolahnya...</td>\n",
              "      <td>[tidak, lah, kakak, capek, lah, entar, sekolah...</td>\n",
              "      <td>[kakak, capek, entar, sekolahnya, main, game, ...</td>\n",
              "      <td>[kakak, capek, entar, sekolah, main, game, ya,...</td>\n",
              "      <td>kakak capek entar sekolah main game ya aja enak</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10128 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  \\\n",
              "0      Udah 2022, SMA saya juga ga full day, dan tete...   \n",
              "1      Saya sampai demam karena lamanya sekolah dan p...   \n",
              "2      Halo Haikal, saya dari masa depan tepatnya tah...   \n",
              "3                              Gak bisa nonton anime eek   \n",
              "4      <a href=\"http://www.youtube.com/results?search...   \n",
              "...                                                  ...   \n",
              "10123  Gua kelas 9 (3 smp) menurut gua sih sekolah fu...   \n",
              "10124  kalau full day nya versi kaya ko   sih saya se...   \n",
              "10125         bisa sih full day asal banyaknya istirahat   \n",
              "10126  kalaw saya sekolah agam islam jadi apa termasu...   \n",
              "10127  Gak lah bang capek lah entar sekolahnya belum ...   \n",
              "\n",
              "                                            case folding  \\\n",
              "0      udah 2022, sma saya juga ga full day, dan tete...   \n",
              "1      saya sampai demam karena lamanya sekolah dan p...   \n",
              "2      halo haikal, saya dari masa depan tepatnya tah...   \n",
              "3                              gak bisa nonton anime eek   \n",
              "4      <a href=\"http://www.youtube.com/results?search...   \n",
              "...                                                  ...   \n",
              "10123  gua kelas 9 (3 smp) menurut gua sih sekolah fu...   \n",
              "10124  kalau full day nya versi kaya ko   sih saya se...   \n",
              "10125         bisa sih full day asal banyaknya istirahat   \n",
              "10126  kalaw saya sekolah agam islam jadi apa termasu...   \n",
              "10127  gak lah bang capek lah entar sekolahnya belum ...   \n",
              "\n",
              "                                          data cleansing  \\\n",
              "0      udah sma saya juga ga full day dan tetep aja g...   \n",
              "1      saya sampai demam karena lamanya sekolah dan p...   \n",
              "2      halo haikal saya dari masa depan tepatnya tahu...   \n",
              "3                              gak bisa nonton anime eek   \n",
              "4      yang aku suka adalah ko contohin untuk nyari i...   \n",
              "...                                                  ...   \n",
              "10123  gua kelas smp menurut gua sih sekolah full day...   \n",
              "10124  kalau full day nya versi kaya ko sih saya setu...   \n",
              "10125         bisa sih full day asal banyaknya istirahat   \n",
              "10126  kalaw saya sekolah agam islam jadi apa termasu...   \n",
              "10127  gak lah bang capek lah entar sekolahnya belum ...   \n",
              "\n",
              "                                            tokenization  \\\n",
              "0      [udah, sma, saya, juga, ga, full, day, dan, te...   \n",
              "1      [saya, sampai, demam, karena, lamanya, sekolah...   \n",
              "2      [halo, haikal, saya, dari, masa, depan, tepatn...   \n",
              "3                        [gak, bisa, nonton, anime, eek]   \n",
              "4      [yang, aku, suka, adalah, ko, contohin, untuk,...   \n",
              "...                                                  ...   \n",
              "10123  [gua, kelas, smp, menurut, gua, sih, sekolah, ...   \n",
              "10124  [kalau, full, day, nya, versi, kaya, ko, sih, ...   \n",
              "10125  [bisa, sih, full, day, asal, banyaknya, istira...   \n",
              "10126  [kalaw, saya, sekolah, agam, islam, jadi, apa,...   \n",
              "10127  [gak, lah, bang, capek, lah, entar, sekolahnya...   \n",
              "\n",
              "                                         normalized_term  \\\n",
              "0      [sudah, sama, saya, juga, tidak, full, day, da...   \n",
              "1      [saya, sampai, demam, karena, lama, sekolah, d...   \n",
              "2      [halo, haikal, saya, dari, masa, depan, tepatn...   \n",
              "3                      [tidak, bisa, nonton, anime, tai]   \n",
              "4      [yang, aku, suka, adalah, kok, contoh, untuk, ...   \n",
              "...                                                  ...   \n",
              "10123  [aku, kelas, sampai, menurut, aku, sih, sekola...   \n",
              "10124  [kalau, full, day, nya, versi, kaya, kok, sih,...   \n",
              "10125  [bisa, sih, full, day, asal, banyaknya, istira...   \n",
              "10126  [kalaw, saya, sekolah, agama, islam, jadi, apa...   \n",
              "10127  [tidak, lah, kakak, capek, lah, entar, sekolah...   \n",
              "\n",
              "                                               stopwords  \\\n",
              "0                                  [aja, kemajuan, aduh]   \n",
              "1                            [demam, sekolah, pelajaran]   \n",
              "2      [halo, tepatnya, sekolah, rumah, wabah, virus,...   \n",
              "3                                   [nonton, anime, tai]   \n",
              "4                   [suka, contoh, mencari, info, komen]   \n",
              "...                                                  ...   \n",
              "10123                                   [kelas, sekolah]   \n",
              "10124  [nya, versi, kaya, setuju, banget, sekolah, pe...   \n",
              "10125                             [banyaknya, istirahat]   \n",
              "10126                     [kalaw, sekolah, agama, islam]   \n",
              "10127  [kakak, capek, entar, sekolahnya, main, game, ...   \n",
              "\n",
              "                                                 stemmed  \\\n",
              "0                                      [aja, maju, aduh]   \n",
              "1                                 [demam, sekolah, ajar]   \n",
              "2      [halo, tepat, sekolah, rumah, wabah, virus, tu...   \n",
              "3                                   [nonton, anime, tai]   \n",
              "4                      [suka, contoh, cari, info, komen]   \n",
              "...                                                  ...   \n",
              "10123                                   [kelas, sekolah]   \n",
              "10124  [nya, versi, kaya, setuju, banget, sekolah, di...   \n",
              "10125                                [banyak, istirahat]   \n",
              "10126                     [kalaw, sekolah, agama, islam]   \n",
              "10127  [kakak, capek, entar, sekolah, main, game, ya,...   \n",
              "\n",
              "                                                    text  \n",
              "0                                          aja maju aduh  \n",
              "1                                     demam sekolah ajar  \n",
              "2             halo tepat sekolah rumah wabah virus tular  \n",
              "3                                       nonton anime tai  \n",
              "4                            suka contoh cari info komen  \n",
              "...                                                  ...  \n",
              "10123                                      kelas sekolah  \n",
              "10124  nya versi kaya setuju banget sekolah didik mur...  \n",
              "10125                                   banyak istirahat  \n",
              "10126                          kalaw sekolah agama islam  \n",
              "10127    kakak capek entar sekolah main game ya aja enak  \n",
              "\n",
              "[10128 rows x 8 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter\n",
        "\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemmed_wrapper(term):\n",
        "    if term == 'setuju':\n",
        "        return 'setuju'\n",
        "    else:\n",
        "        return stemmer.stem(term)\n",
        "\n",
        "# Membuat ketentuan dari stemming\n",
        "term_dict = {}\n",
        "for document in df['normalized_term']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = stemmed_wrapper(term)\n",
        "\n",
        "# Menggunakan stemming\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "df['stemmed'] = df['stopwords'].swifter.apply(get_stemmed_term)\n",
        "\n",
        "\n",
        "def fit_stopwords(text):\n",
        "    text= np.array(text)\n",
        "    text= ' '.join(text)\n",
        "    return text\n",
        "\n",
        "df['text']=df['stemmed'].apply(lambda x: fit_stopwords(x))\n",
        "df.to_excel(\"output/bahasa.xlsx\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /Users/galkasoft/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index 9\n",
            "index 28\n",
            "index 77\n",
            "index 81\n",
            "index 91\n",
            "index 97\n",
            "index 108\n",
            "index 110\n",
            "index 135\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Terjemahkan teks ke bahasa Inggris\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     translated \u001b[38;5;241m=\u001b[39m GoogleTranslator(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Hitung sentimen menggunakan VADER\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     vader \u001b[38;5;241m=\u001b[39m sia\u001b[38;5;241m.\u001b[39mpolarity_scores(translated)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Download VADER lexicon jika belum pernah dilakukan\n",
        "nltk.download('vader_lexicon')\n",
        "df = pd.read_excel('output/bahasa.xlsx')\n",
        "\n",
        "# Inisialisasi SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Inisialisasi Google Translator\n",
        "translator = Translator()\n",
        "sentiment_data = []\n",
        "sentiment_positif = []\n",
        "sentiment_negatif = []\n",
        "sentiment_netral = []\n",
        "\n",
        "# Loop untuk menerjemahkan dan melakukan sentiment analysis\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        # Terjemahkan teks ke bahasa Inggris\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(row['text'])\n",
        "        time.sleep(1)\n",
        "        # Hitung sentimen menggunakan VADER\n",
        "        vader = sia.polarity_scores(translated)\n",
        "        sentiment = vader['compound']\n",
        "       \n",
        "        # Tentukan label sentimen\n",
        "        if sentiment > 0:\n",
        "            label = 'positif'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_positif.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        elif sentiment < 0:\n",
        "            label = 'negatif'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_negatif.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        else:\n",
        "            label = 'netral'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_netral.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        \n",
        "        sentiment_data.append({\n",
        "            'Text': row['text'],\n",
        "            'Translated': translated,\n",
        "            'Sentimen': sentiment,\n",
        "            'Label': label\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print('index', index)\n",
        "\n",
        "        \n",
        "# Convert lists to DataFrames\n",
        "df_sentiment_positif = pd.DataFrame(sentiment_positif)\n",
        "df_sentiment_negatif = pd.DataFrame(sentiment_negatif)\n",
        "df_sentiment_netral = pd.DataFrame(sentiment_netral)\n",
        "data = pd.DataFrame(sentiment_data)\n",
        "\n",
        "# Print the DataFrame to check results\n",
        "\n",
        "# Menyimpan hasil ke Excel\n",
        "df_sentiment_positif.to_excel(\"output/lexicon/lexicon_positif.xlsx\", index=False)\n",
        "df_sentiment_negatif.to_excel(\"output/lexicon/lexicon_negatif.xlsx\", index=False)\n",
        "df_sentiment_netral.to_excel(\"output/lexicon/lexicon_netral.xlsx\", index=False)\n",
        "data.to_excel(\"output/lexicon/sentiment_analysis.xlsx\", index=False)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Sentiment Analysis : Melakukan analisis sentiment pada data yang sudah di latih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "data  = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "# Inisialisasi objek TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Melakukan pembobotan TF-IDF pada data pelatihan\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(data['Text'])\n",
        "\n",
        "# Melakukan pembobotan TF-IDF pada data uji dengan menggunakan vocabulary yang sama dengan data pelatihan\n",
        "tfidf_test = tfidf_vectorizer.transform(data['Text'])\n",
        "\n",
        "# Tampilkan hasil pembobotan\n",
        "print(\"Matriks TF-IDF untuk data pelatihan:\")\n",
        "print(tfidf_train)\n",
        "\n",
        "print(\"\\nMatriks TF-IDF untuk data uji:\")\n",
        "print(tfidf_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "data = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "\n",
        "# inisialisasi TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# ubah teks menjadi vektor\n",
        "X = vectorizer.fit_transform(data['Translated'])\n",
        "\n",
        "# MODEL TRAINING, SVM -------\n",
        "# pada tahap ini kita akan melakukan training menggunakan algoritma SVM\n",
        "# SVM digunakan jika kita memiliki data yang tidak terlalu besar, di samping itu SVM tidak membutuhkan\n",
        "# sumber daya komputasi yang besar dibandingkan ANN, sehingga cocok untuk data yang tidak terlalu besar\n",
        "\n",
        "# inisialisasi model SVM\n",
        "svm = SVC()\n",
        "\n",
        "# latih model SVM\n",
        "svm.fit(X, data['Label'])\n",
        "\n",
        "# MODEL EVALUATION -------\n",
        "# pada tahap ini kita akan melakukan evaluasi model yang sudah kita latih\n",
        "\n",
        "# prediksi label menggunakan SVM\n",
        "y_pred_svm = svm.predict(X)\n",
        "\n",
        "# hitung akurasi\n",
        "accuracy_svm = np.mean(y_pred_svm == data['Label'])\n",
        "\n",
        "# tampilkan hasil akurasi\n",
        "print('Akurasi SVM:', accuracy_svm)\n",
        "# print(data[['Sentimen', 'Label']])\n",
        "\n",
        "# SAVE MODEL -------\n",
        "# setelah model berhasil dilatih, kita perlu menyimpan model ke dalam file\n",
        "# file yang kita simpan di sini adalah model dan vectorizer yang sudah kita latih\n",
        "# model digunakan untuk melakukan prediksi atau analisis sentimen, sedangkan vectorizer digunakan untuk\n",
        "# mengubah teks menjadi vektor numerik yang dapat digunakan oleh algoritma SVM\n",
        "\n",
        "\n",
        "# Calculating the number of each sentiment\n",
        "jumlah_negatif = sum(data['Label'] == 'negatif')\n",
        "jumlah_positif = sum(data['Label'] == 'positif')\n",
        "jumlah_netral = sum(data['Label'] == 'netral')\n",
        "total_data = len(df)\n",
        "\n",
        "print(\"Negatif:\", jumlah_negatif, \", Positif:\", jumlah_positif, \", Netral:\", jumlah_netral, \", Total:\", total_data)\n",
        "\n",
        "# Calculate the number of training and testing data\n",
        "jumlah_data_training = int(0.8 * total_data)\n",
        "jumlah_data_testing = total_data - jumlah_data_training\n",
        "\n",
        "# Calculate the percentage of training and testing data\n",
        "persentase_training = (jumlah_data_training / total_data) * 100\n",
        "persentase_testing = (jumlah_data_testing / total_data) * 100\n",
        "\n",
        "# Calculating the number of positive sentiment data used for training and testing\n",
        "jumlah_positif_training = len(data[(data['Label'] == 'positif') & (data.index < jumlah_data_training)])\n",
        "jumlah_positif_testing = len(data[(data['Label'] == 'positif') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "# Calculating the number of positive sentiment data used for training and testing\n",
        "jumlah_netral_training = len(data[(data['Label'] == 'netral') & (data.index < jumlah_data_training)])\n",
        "jumlah_netral_testing = len(data[(data['Label'] == 'netral') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "# Calculating the number of negative sentiment data used for training and testing\n",
        "jumlah_negatif_training = len(data[(data['Label'] == 'negatif') & (data.index < jumlah_data_training)])\n",
        "jumlah_negatif_testing = len(data[(data['Label'] == 'negatif') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "len_data = 10  # Adjust this based on your data length\n",
        "\n",
        "print(\"\\nPembagian Dataset\")\n",
        "print(\"-\" * 100)\n",
        "print(\"{:<{len_data}} | {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\n",
        "    \"Sentiment \", \"Jumlah\", f\"Data Training ({persentase_training:.2f}%)\",\n",
        "    f\"Data Testing ({persentase_testing:.2f}%)\", len_data=len_data))\n",
        "print(\"-\" * 100)\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Negatif\", jumlah_negatif, jumlah_negatif_training, jumlah_negatif_testing, len_data=len_data + 1))\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Netral\", jumlah_netral, jumlah_netral_training, jumlah_netral_testing, len_data=len_data + 1))\n",
        "\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Positif\", jumlah_positif, jumlah_positif_training, jumlah_positif_testing, len_data=len_data + 1))\n",
        "print(\"-\" * 100)\n",
        "# print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Netral\", jumlah_netral, jumlah_data_training, jumlah_data_testing, len_data=len_data + 1))\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Total\", jumlah_negatif + jumlah_positif+ jumlah_netral, jumlah_negatif_training + jumlah_positif_training+ jumlah_netral_training, jumlah_negatif_testing + jumlah_positif_testing+ jumlah_netral_testing, len_data=len_data + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - WORDCLOUD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# Load the data\n",
        "data = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "# Filter for neutral sentiment\n",
        "neutral_data = data[data['Label'] == 'netral']\n",
        "\n",
        "# Combine all the text from the neutral sentiment rows\n",
        "combined_text = ' '.join(neutral_data['Text'].astype(str))\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - BAR CHART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_axis = ['positif', 'netral', 'negatif']\n",
        "y_axis = [(sum(df['Label']=='positif')), (sum(df['Label']=='netral')), (sum(df['Label']=='negatif'))]\n",
        "c = ['palegreen','khaki','tomato']\n",
        "plt.bar(x_axis, y_axis, color = c)\n",
        "plt.title('Bar Chart Sebaran Jumlah Data 3 Kelas Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - PIE CHART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Membuat PieChart\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Pie Chart Persentase Sentimen\", fontsize=12)\n",
        "chart = plt.pie(data.Label.value_counts(),explode=(0.025,0.025,0.025), \n",
        "            labels=data.Label.value_counts().index, \n",
        "            colors=['palegreen','tomato','khaki'],  \n",
        "            autopct='%1.1f%%', startangle=180)\n",
        "plt.show()\n",
        "\n",
        "data.to_excel(\"output/labeled.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - MATRIX CONFUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = pd.read_excel('output/labeled.xlsx')\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split into features and labels\n",
        "y = df.Label.values\n",
        "x = df.Translated.values\n",
        "\n",
        "# Ensure there are more than one unique class\n",
        "unique_labels = df['Label'].unique()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "\n",
        "class_counts = df['Label'].value_counts()\n",
        "print(\"Class Counts:\")\n",
        "print(class_counts)\n",
        "\n",
        "if len(unique_labels) < 2:\n",
        "    raise ValueError(\"There should be more than one unique class for SVM training.\")\n",
        "\n",
        "# Handle imbalanced classes (if necessary)\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df['Label'] == class_counts.idxmax()]\n",
        "df_minority = df[df['Label'] == class_counts.idxmin()]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Split the balanced data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_balanced['Translated'], df_balanced['Label'], \n",
        "                                                    test_size=0.1, random_state=1, shuffle=True)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), binary=True, stop_words='english')\n",
        "vectorizer.fit(list(x_train) + list(x_test))\n",
        "\n",
        "x_train_vec = vectorizer.transform(x_train)\n",
        "x_test_vec = vectorizer.transform(x_test)\n",
        "print(x_train_vec.shape)\n",
        "print(x_test_vec.shape)\n",
        "\n",
        "# Linear SVM Classifier\n",
        "linear = svm.SVC(kernel='linear', C=1)\n",
        "linear.fit(x_train_vec, y_train)\n",
        "linear_pred = linear.predict(x_test_vec)\n",
        "score_linear = accuracy_score(linear_pred, y_test)\n",
        "print(\"Accuracy with Linear SVM: \", score_linear * 100, '%')\n",
        "\n",
        "# RBF SVM Classifier\n",
        "rbf = svm.SVC(kernel='rbf', gamma='scale', C=1)\n",
        "rbf.fit(x_train_vec, y_train)\n",
        "rbf_pred = rbf.predict(x_test_vec)\n",
        "score_rbf = accuracy_score(rbf_pred, y_test)\n",
        "print(\"Accuracy with RBF SVM: \", score_rbf * 100, '%')\n",
        "\n",
        "# Polynomial SVM Classifier\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1)\n",
        "poly.fit(x_train_vec, y_train)\n",
        "poly_pred = poly.predict(x_test_vec)\n",
        "score_poly = accuracy_score(poly_pred, y_test)\n",
        "print(\"Accuracy with Polynomial SVM: \", score_poly * 100, '%')\n",
        "\n",
        "# Print confusion matrix for Linear SVM\n",
        "print(\"Confusion Matrix for Linear SVM:\")\n",
        "print(confusion_matrix(y_test, linear_pred))\n",
        "\n",
        "# Confusion matrix with all classes ('positif', 'netral', 'negatif')\n",
        "cm = confusion_matrix(y_test, linear_pred, labels=['positif', 'netral', 'negatif'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['positif', 'netral', 'negatif'])\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix (Linear SVM)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix for RBF SVM\n",
        "print(\"Confusion Matrix for RBF SVM:\")\n",
        "print(confusion_matrix(y_test, rbf_pred))\n",
        "\n",
        "# Confusion matrix with all classes ('positif', 'netral', 'negatif')\n",
        "cm = confusion_matrix(y_test, rbf_pred, labels=['positif', 'netral', 'negatif'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['positif', 'netral', 'negatif'])\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix (RBF SVM)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix for Polynomial SVM\n",
        "print(\"Confusion Matrix for Polynomial SVM:\")\n",
        "print(confusion_matrix(y_test, poly_pred))\n",
        "\n",
        "# Confusion matrix with all classes ('positif', 'netral', 'negatif')\n",
        "cm = confusion_matrix(y_test, poly_pred, labels=['positif', 'netral', 'negatif'])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['positif', 'netral', 'negatif'])\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix (Polynomial SVM)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TAHAP - LAPORAN KLASIFIKASI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Menginisialisasi model SVM dengan kernel linear\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Melatih model SVM menggunakan data pelatihan yang telah diboboti dengan TF-IDF dan label sentimen\n",
        "svm_classifier.fit(tfidf_train, data['Label'])\n",
        "\n",
        "# Memprediksi label sentimen pada data uji menggunakan model SVM yang telah dilatih\n",
        "predictions = svm_classifier.predict(tfidf_test)\n",
        "\n",
        "# Menampilkan laporan klasifikasi dengan zero_division diatur\n",
        "print(\"Laporan Klasifikasi:\")\n",
        "print(classification_report(data['Label'], predictions, zero_division=0))\n",
        "# from sklearn.metrics import classification_report\n",
        "# print(classification_report(data['Label'],y_test, linear_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
