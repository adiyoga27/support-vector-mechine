{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANALISIS SENTIMEN TERHADAP KEBIJAKAN FULL DAY SCHOOL DENGAN METODE SUPPORT VECTOR MACHINE (SVM)\n",
        "Aplikasi ini menggunakan methode SVM. ada pun library yang harus digunakan antara lainnya :  \n",
        "1. Install pandas: pip install pandas\n",
        "2. Install numpy: pip install numpy\n",
        "3. Install matplotlib: pip install matplotlib\n",
        "4. Install openpyxl: pip install openpyxl\n",
        "5. Install string: pip install string\n",
        "6. Install re: pip install re\n",
        "7. Install nltk: pip install nltk\n",
        "8. Install swifter: pip install swifter\n",
        "9. Install deep_translator: pip install deep-translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAiN_lrwlqM3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import openpyxl\n",
        "import string\n",
        "import re #regex library\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import swifter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MENYIAPKAN DATA LATIH : Mengambil data latih yang sudah disiapkan dalam bentuk Excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "Qqcixw4WtZz1",
        "outputId": "f871b7d4-070a-4efa-df51-e35673614797"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('assets/data_latih.xlsx')\n",
        "df=pd.DataFrame(data[['tweet']])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CASE FOLDING: Melakukan case folding dari hasil tweet, dimana case folding mengubah semua huruf menjadi lowercase (huruf Kecil)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "I9lZLNQcuY31",
        "outputId": "4281ec8b-676b-4733-f32a-bfcbf1b4498d"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['case folding'] = df['tweet'].str.lower()\n",
        "df.to_excel(\"output/processing/data cleansing.xlsx\", index=False)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLEANING DATA\n",
        "1. Menghapus semua tanda baca dan karakter yang bukan huruf.\n",
        "2. Menghapus angka yang mungkin merupakan kata-kata yang penting.\n",
        "3. Menghapus whitespace yang ada di awal dan akhir tweet.\n",
        "4. Menghapus whitespace yang ada di antara kata.\n",
        "5. Menghapus kata-kata yang kata sendiri."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CxKfC6gs0gmF",
        "outputId": "a891ede7-bc8e-47c9-dbfc-0eddbc314030"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_tweet_special(text):\n",
        "    # Mengcheck bahwa data merupakan string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Jika bukan string, akan kembalikan data kosong\n",
        "    \n",
        "\n",
        "    # Menghilangkan tab, line baru, and backslash karakter\n",
        "    text = text.replace('\\t', \" \").replace('\\n', \" \").replace('\\\\', \" \").replace(\"-\", \" \").replace(\",\", \" \").replace(\".\", \" \")\n",
        "\n",
        "    # Menghilangkan non-ASCII characters (emoticons, Chinese characters, etc.)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "\n",
        "    # Menghilangkan mentions, links, and hashtags\n",
        "    text = re.sub(r'[@#][A-Za-z0-9]+|https?://\\S+', \" \", text)\n",
        "\n",
        "    # Menghilangkan extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk menghapus tag HTML\n",
        "def remove_html_tags(text):\n",
        "    # Ganti <br> dengan spasi\n",
        "    text = text.replace(\"<br>\", \" \").replace(\"<br/>\", \" \").replace(\"<p>\", \" \").replace(\"</p>\", \" \").replace(\"<h1>\", \" \").replace(\"</h1>\", \" \").replace(\"<h2>\",\" \").replace(\"<h3>\",\" \").replace(\"<h4>\",\" \").replace(\"href\", \" \")\n",
        "    # Hapus tag HTML lainnya\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text()\n",
        "    # Ganti spasi berlebih dengan satu spasi\n",
        "    clean_text = ' '.join(clean_text.split())\n",
        "    return clean_text\n",
        "\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "#Menghilangkan  punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "#Menghilangkan whitespace leading & trailing\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "#Menghilangkan kelebihan whitespace into single whitespace\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "def remove_single_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "df = pd.read_excel('output/processing/data cleansing.xlsx')\n",
        "\n",
        "df['data cleansing'] = df['case folding'].apply(remove_tweet_special)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_html_tags)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_number)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_punctuation)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_whitespace_LT)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_whitespace_multiple)\n",
        "\n",
        "df['data cleansing'] = df['data cleansing'].apply(remove_single_char)\n",
        "\n",
        "df.to_excel(\"output/processing/cleansing.xlsx\", index=False)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Tokenization : \n",
        "Melakukan tokenisasi kata-kata pada data yang sudah dibersihkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "l_3b01k39TTw",
        "outputId": "717fa78f-efa7-46cc-c9a5-ed5bc77e6b52"
      },
      "outputs": [],
      "source": [
        "# Mengunduh  NLTK data untuk tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def tokenization(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "\n",
        "df['tokenization'] = df['data cleansing'].apply(tokenization)\n",
        "df.to_excel(\"output/processing/tokenization.xlsx\", index=False)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenization : Melakukan normalisasi kata-kata yang telah di-tokenisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "H-uVu9W7CePT",
        "outputId": "9d761448-5776-4d31-e119-117d64804b64"
      },
      "outputs": [],
      "source": [
        "normalized_word = pd.read_excel(\"assets/normalisasi.xlsx\")\n",
        "\n",
        "# Membuat kamus untuk normalisasi\n",
        "normalized_word_dict = {}\n",
        "for index, row in normalized_word.iterrows():\n",
        "    normalized_word_dict[row[0]] = row[1]  \n",
        "\n",
        "# fungsi tokenisasi\n",
        "def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Fungsi normalisasi\n",
        "def normalized_term(document):\n",
        "    return [normalized_word_dict.get(term, term) for term in document]\n",
        "\n",
        "df['normalized_term'] = df['tokenization'].apply(normalized_term)\n",
        "df.to_excel(\"output/processing/normalized_term.xlsx\", index=False)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stopwords : Melakukan stopwords pada data normalisasi yang umum dipakai dalam bahasa Indonesia disebutkan dalam file stopwords.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "kDjQdwVg_vNG",
        "outputId": "6c3edffd-e160-4f79-a916-58ba9ba1cb32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the necessary NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Menggunakan Indonesian stopwords\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# Bisa menambahkan stopword yang dibutuhkan ke list stopwords\n",
        "list_stopwords.extend([\n",
        "    'full', 'day',\n",
        "    'nik', 'ais', 'ih', 'kuea', 'ndes', 'tk', 'arg', 'hhhh', 'wuakakak',\n",
        "    'gtth', 'wowww', 'apeeee', 'Aksjsjsk', 'alaee', 'koq', 'salengpraew',\n",
        "    'rukkhadevata', 'zeon', 'vivienne', 'yaam', 'woyy', 'ykwi', 'auff',\n",
        "    'ue', 'hoek', 'hayo', 'chnmn', 'hahahah', 'haaaaaa', 'din', 'woy',\n",
        "    'ndeer', 'lalalala', 'wkwkwkwkwkkw', 'woyyy', 'dih', 'den', 'hehew',\n",
        "    'etdah', 'beeeuh', 'wahh', 'heheee', 'hhaaha', 'waaaaa', 'oakilah',\n",
        "    'haaaahh', 'huft', 'ai', 'et', 'acha', 'hokyahokya', 'hahahihi',\n",
        "    'yl', 'wihh', 'hahahaa', 'hhhh', 'def', 'ayom', 'ser', 'duh',\n",
        "    'heuheueheu', 'huwaaaaaa', 'yalah', 'mww', 'cekabia', 'dikatara',\n",
        "    'angganara', 'krtsk', 'woee', 'ndi', 'ohh', 'www', 'aee', 'huaaaa',\n",
        "    'gn', 'hahahah', 'nd', 'ema', 'ceratops', 'pasuk', 'ygy', 'repp',\n",
        "    'gais', 'hadehhh', 'walah', 'hahah', 'paa', 'awkwkwk', 'wkwkk',\n",
        "    'wkwkw', 'wkwkwkwkwkwah','haikal', 'wkwkwkw', 'baceprot', 'sksksk', 'heheh',\n",
        "    'brooo', 'dbd', 'aeee', 'weeeh', 'wehh', 'milta', 'hsnah', 'swsg',\n",
        "    'hemm', 'xda', 'yara', 'ohh', 'heh', 'kle', 'acy', 'hayooo',\n",
        "    'hahahahaha', 'balablablabla', 'lai', 'loj', 'itine', 'heehehe',\n",
        "    'kwkwk', 'kwkwkwkwwkwk', 'waaa', 'demending', 'pali', 'eeh',\n",
        "    'dlsb', 'cooooy', 'hehehehe', 'adjem', 'aih', 'syar', 'wkwkk',\n",
        "    'aowkwkwk', 'walah', 'euy', 'der', 'hahaa', 'hesteg', 'hmmmmtar',\n",
        "    'gtideologi', 'ab', 'owkwkwkwk', 'dncw', 'sloga', 'jo', 'jengjenggg',\n",
        "    'anuanu', 'caw', 'ehehheheh', 'hlaa', 'hahahihi', 'ckckckck',\n",
        "    'sich', 'pakin', 'mmarkpkk', 'ponponpon', 'kyary', 'pamyu',\n",
        "    'laaahhh', 'cp', 'duhhh', 'eno', 'lise', 'bi', 'ieu', 'poho',\n",
        "    'boga', 'imah', 'keur', 'ulin', 'kwkwkw', 'ehheh', 'gryli',\n",
        "    'oalah', 'prekk', 'hehh', 'cere', 'ekekekek', 'chco', 'nganu',\n",
        "    'wkwkkwkwkwkwkw', 'zell', 'awowkwkwkwk', 'kinyis', 'pus', 'yng',\n",
        "    'yg', 'yang', 'wkwoswkwo', 'wkwkwkwkwkwk', 'ahahha', 'weeeeh',\n",
        "    'hah', 'nuuuuuuuuuuuuuuuuuuuuuuuuuuuuu', 'hong', 'jay', 'haikyuu', 'nderrr',\n",
        "    'omtanteuwaksodara', 'ahsajkakaka', 'kwkwkwk', 'derrr',\n",
        "    'wwkwkwkw', 'hadehh', 'aaaaa', 'heeh', 'dem', 'ocaaa',\n",
        "    'wo', 'prenup', 'dihhh', 'cokk', 'imho', 'chenle',\n",
        "    'jsdieksisnisawikwok', 'hahahahahahaha', 'bam', 'yowohh',\n",
        "    'lau', 'boiiiii', 'gih', 'beuhhh', 'wkw', 'wkwkwkw',\n",
        "    'dooong', 'oalaaaa', 'sinoeng', 'wkekwk', 'nyai',\n",
        "    'cai', 'anw', 'tjuyyy', 'hanss', 'mh', 'ih',\n",
        "    'widihh', 'cy', 'eeeee', 'gi', 'luat', 'laaaaa',\n",
        "    'cam', 'lancau', 'tuch', 'kun', 'uhhhh', 'chuakssss',\n",
        "    'oiyaa', 'hadeuhhhh', 'wkwkwkwwk', 'hehehee', 'nk', 'sih','nih',\n",
        "    'lak', 'qwq', 'oneesan', 'eeehmmm', 'am', 'wkwk', 'hahaha','zellnya', 'ea' ,'ealah','quot'\n",
        "])\n",
        "\n",
        "# Mengisi data manual stopwords ke excel\n",
        "txt_stopword = pd.read_csv(\"assets/stopwordbahasa.txt\", names=[\"stopwords\"], header=None)\n",
        "list_stopwords.extend(txt_stopword[\"stopwords\"])\n",
        "\n",
        "# Mengconvert data stopword\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "# Fungsi untuk menghilangkan stopwords\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "df['stopwords'] = df['normalized_term'].apply(stopwords_removal)\n",
        "df.to_excel(\"output/processing/stopwords.xlsx\", index=False)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stemming : Melakukan stemming pada data stopwords yang umum dipakai dalam bahasa Indonesia menggunakan library Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[228], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 35\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstemmed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/bahasa.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m df\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[228], line 35\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     32\u001b[0m     text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 35\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfit_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     36\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/bahasa.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m df\n",
            "Cell \u001b[1;32mIn[228], line 31\u001b[0m, in \u001b[0;36mfit_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_stopwords\u001b[39m(text):\n\u001b[1;32m---> 31\u001b[0m     text\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(text)\n\u001b[0;32m     32\u001b[0m     text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter\n",
        "\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemmed_wrapper(term):\n",
        "    if term == 'setuju':\n",
        "        return 'setuju'\n",
        "    else:\n",
        "        return stemmer.stem(term)\n",
        "\n",
        "# Membuat ketentuan dari stemming\n",
        "term_dict = {}\n",
        "for document in df['normalized_term']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = stemmed_wrapper(term)\n",
        "\n",
        "# Menggunakan stemming\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "df['stemmed'] = df['stopwords'].swifter.apply(get_stemmed_term)\n",
        "\n",
        "\n",
        "def fit_stopwords(text):\n",
        "    text= np.array(text)\n",
        "    text= ' '.join(text)\n",
        "    return text\n",
        "\n",
        "df['text']=df['stemmed'].apply(lambda x: fit_stopwords(x))\n",
        "df.to_excel(\"output/bahasa.xlsx\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Download VADER lexicon jika belum pernah dilakukan\n",
        "nltk.download('vader_lexicon')\n",
        "df = pd.read_excel('output/bahasa.xlsx')\n",
        "\n",
        "# Inisialisasi SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Inisialisasi Google Translator\n",
        "translator = Translator()\n",
        "sentiment_data = []\n",
        "sentiment_positif = []\n",
        "sentiment_negatif = []\n",
        "sentiment_netral = []\n",
        "\n",
        "# Loop untuk menerjemahkan dan melakukan sentiment analysis\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        # Terjemahkan teks ke bahasa Inggris\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(row['text'])\n",
        "        time.sleep(1)\n",
        "        # Hitung sentimen menggunakan VADER\n",
        "        vader = sia.polarity_scores(translated)\n",
        "        sentiment = vader['compound']\n",
        "       \n",
        "        # Tentukan label sentimen\n",
        "        if sentiment > 0:\n",
        "            label = 'positif'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_positif.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        elif sentiment < 0:\n",
        "            label = 'negatif'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_negatif.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        else:\n",
        "            label = 'netral'\n",
        "            df.at[index, 'Label'] = label\n",
        "            sentiment_netral.append({\n",
        "                'Text': row['text'],\n",
        "                'Translated': translated,\n",
        "                'Sentimen': sentiment,\n",
        "                'Label': label\n",
        "            })\n",
        "        \n",
        "        sentiment_data.append({\n",
        "            'Text': row['text'],\n",
        "            'Translated': translated,\n",
        "            'Sentimen': sentiment,\n",
        "            'Label': label\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print('Error processing data at index', index, ':', e, ':' ,row['text'])\n",
        "        # df.at[index, 'Sentimen'] = 0\n",
        "        # df.at[index, 'Label'] = 'netral'\n",
        "        # sentiment_netral.append({\n",
        "        #     'Text': row['text'],\n",
        "        #     'Translated': translated,\n",
        "        #     'Sentimen': 0,\n",
        "        #     'Label': 'netral'\n",
        "        # })\n",
        "        # sentiment_data.append({\n",
        "        #     'Text': row['text'],\n",
        "        #     'Translated': translated,\n",
        "        #     'Sentimen': 0,\n",
        "        #     'Label': 'netral'\n",
        "        # })\n",
        "        \n",
        "# Convert lists to DataFrames\n",
        "df_sentiment_positif = pd.DataFrame(sentiment_positif)\n",
        "df_sentiment_negatif = pd.DataFrame(sentiment_negatif)\n",
        "df_sentiment_netral = pd.DataFrame(sentiment_netral)\n",
        "data = pd.DataFrame(sentiment_data)\n",
        "\n",
        "# Print the DataFrame to check results\n",
        "\n",
        "# Menyimpan hasil ke Excel\n",
        "df_sentiment_positif.to_excel(\"output/lexicon/lexicon_positif.xlsx\", index=False)\n",
        "df_sentiment_negatif.to_excel(\"output/lexicon/lexicon_negatif.xlsx\", index=False)\n",
        "df_sentiment_netral.to_excel(\"output/lexicon/lexicon_netral.xlsx\", index=False)\n",
        "data.to_excel(\"output/lexicon/sentiment_analysis.xlsx\", index=False)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Sentiment Analysis : Melakukan analisis sentiment pada data yang sudah di latih"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "data  = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "# Inisialisasi objek TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Melakukan pembobotan TF-IDF pada data pelatihan\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(data['Text'])\n",
        "\n",
        "# Melakukan pembobotan TF-IDF pada data uji dengan menggunakan vocabulary yang sama dengan data pelatihan\n",
        "tfidf_test = tfidf_vectorizer.transform(data['Text'])\n",
        "\n",
        "# Tampilkan hasil pembobotan\n",
        "print(\"Matriks TF-IDF untuk data pelatihan:\")\n",
        "print(tfidf_train)\n",
        "\n",
        "print(\"\\nMatriks TF-IDF untuk data uji:\")\n",
        "print(tfidf_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "# inisialisasi TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# ubah teks menjadi vektor\n",
        "X = vectorizer.fit_transform(data['Translated'])\n",
        "\n",
        "# MODEL TRAINING, SVM -------\n",
        "# pada tahap ini kita akan melakukan training menggunakan algoritma SVM\n",
        "# SVM digunakan jika kita memiliki data yang tidak terlalu besar, di samping itu SVM tidak membutuhkan\n",
        "# sumber daya komputasi yang besar dibandingkan ANN, sehingga cocok untuk data yang tidak terlalu besar\n",
        "\n",
        "# inisialisasi model SVM\n",
        "svm = SVC()\n",
        "\n",
        "# latih model SVM\n",
        "svm.fit(X, data['Label'])\n",
        "\n",
        "# MODEL EVALUATION -------\n",
        "# pada tahap ini kita akan melakukan evaluasi model yang sudah kita latih\n",
        "\n",
        "# prediksi label menggunakan SVM\n",
        "y_pred_svm = svm.predict(X)\n",
        "\n",
        "# hitung akurasi\n",
        "accuracy_svm = np.mean(y_pred_svm == data['Label'])\n",
        "\n",
        "# tampilkan hasil akurasi\n",
        "print('Akurasi SVM:', accuracy_svm)\n",
        "# print(data[['Sentimen', 'Label']])\n",
        "\n",
        "# SAVE MODEL -------\n",
        "# setelah model berhasil dilatih, kita perlu menyimpan model ke dalam file\n",
        "# file yang kita simpan di sini adalah model dan vectorizer yang sudah kita latih\n",
        "# model digunakan untuk melakukan prediksi atau analisis sentimen, sedangkan vectorizer digunakan untuk\n",
        "# mengubah teks menjadi vektor numerik yang dapat digunakan oleh algoritma SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculating the number of each sentiment\n",
        "jumlah_negatif = sum(data['Label'] == 'negatif')\n",
        "jumlah_positif = sum(data['Label'] == 'positif')\n",
        "jumlah_netral = sum(data['Label'] == 'netral')\n",
        "total_data = len(df)\n",
        "\n",
        "print(\"Negatif:\", jumlah_negatif, \", Positif:\", jumlah_positif, \", Netral:\", jumlah_netral, \", Total:\", total_data)\n",
        "\n",
        "# Calculate the number of training and testing data\n",
        "jumlah_data_training = int(0.8 * total_data)\n",
        "jumlah_data_testing = total_data - jumlah_data_training\n",
        "\n",
        "# Calculate the percentage of training and testing data\n",
        "persentase_training = (jumlah_data_training / total_data) * 100\n",
        "persentase_testing = (jumlah_data_testing / total_data) * 100\n",
        "\n",
        "# Calculating the number of positive sentiment data used for training and testing\n",
        "jumlah_positif_training = len(data[(data['Label'] == 'positif') & (data.index < jumlah_data_training)])\n",
        "jumlah_positif_testing = len(data[(data['Label'] == 'positif') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "# Calculating the number of positive sentiment data used for training and testing\n",
        "jumlah_netral_training = len(data[(data['Label'] == 'netral') & (data.index < jumlah_data_training)])\n",
        "jumlah_netral_testing = len(data[(data['Label'] == 'netral') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "# Calculating the number of negative sentiment data used for training and testing\n",
        "jumlah_negatif_training = len(data[(data['Label'] == 'negatif') & (data.index < jumlah_data_training)])\n",
        "jumlah_negatif_testing = len(data[(data['Label'] == 'negatif') & (data.index >= jumlah_data_training)])\n",
        "\n",
        "len_data = 10  # Adjust this based on your data length\n",
        "\n",
        "print(\"\\nPembagian Dataset\")\n",
        "print(\"-\" * 100)\n",
        "print(\"{:<{len_data}} | {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\n",
        "    \"Sentiment \", \"Jumlah\", f\"Data Training ({persentase_training:.2f}%)\",\n",
        "    f\"Data Testing ({persentase_testing:.2f}%)\", len_data=len_data))\n",
        "print(\"-\" * 100)\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Negatif\", jumlah_negatif, jumlah_negatif_training, jumlah_negatif_testing, len_data=len_data + 1))\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Netral\", jumlah_netral, jumlah_netral_training, jumlah_netral_testing, len_data=len_data + 1))\n",
        "\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Positif\", jumlah_positif, jumlah_positif_training, jumlah_positif_testing, len_data=len_data + 1))\n",
        "print(\"-\" * 100)\n",
        "# print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Netral\", jumlah_netral, jumlah_data_training, jumlah_data_testing, len_data=len_data + 1))\n",
        "print(\"{:<{len_data}}| {:<8}| {:<{len_data}}| {:<{len_data}}\".format(\"Total\", jumlah_negatif + jumlah_positif+ jumlah_netral, jumlah_negatif_training + jumlah_positif_training+ jumlah_netral_training, jumlah_negatif_testing + jumlah_positif_testing+ jumlah_netral_testing, len_data=len_data + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# Load the data\n",
        "data = pd.read_excel('output/lexicon/sentiment_analysis.xlsx')\n",
        "# Filter for neutral sentiment\n",
        "neutral_data = data[data['Label'] == 'netral']\n",
        "\n",
        "# Combine all the text from the neutral sentiment rows\n",
        "combined_text = ' '.join(neutral_data['Text'].astype(str))\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_axis = ['positif', 'netral', 'negatif']\n",
        "y_axis = [(sum(df['Label']=='positif')), (sum(df['Label']=='netral')), (sum(df['Label']=='negatif'))]\n",
        "c = ['palegreen','khaki','tomato']\n",
        "plt.bar(x_axis, y_axis, color = c)\n",
        "plt.title('Bar Chart Sebaran Jumlah Data 3 Kelas Sentimen')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Membuat PieChart\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Pie Chart Persentase Sentimen\", fontsize=12)\n",
        "chart = plt.pie(data.Label.value_counts(),explode=(0.025,0.025,0.025), \n",
        "            labels=data.Label.value_counts().index, \n",
        "            colors=['palegreen','tomato','khaki'],  \n",
        "            autopct='%1.1f%%', startangle=180)\n",
        "plt.show()\n",
        "\n",
        "data.to_excel(\"output/labeled.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = pd.read_excel('output/labeled.xlsx')\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split into features and labels\n",
        "y = df.Label.values\n",
        "x = df.Translated.values\n",
        "\n",
        "# Ensure there are more than one unique class\n",
        "unique_labels = df['Label'].unique()\n",
        "print(\"Unique Labels:\", unique_labels)\n",
        "\n",
        "class_counts = df['Label'].value_counts()\n",
        "print(\"Class Counts:\")\n",
        "print(class_counts)\n",
        "\n",
        "if len(unique_labels) < 2:\n",
        "    raise ValueError(\"There should be more than one unique class for SVM training.\")\n",
        "\n",
        "# Handle imbalanced classes (if necessary)\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df['Label'] == class_counts.idxmax()]\n",
        "df_minority = df[df['Label'] == class_counts.idxmin()]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Split the balanced data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_balanced['Translated'], df_balanced['Label'], \n",
        "                                                    test_size=0.2, random_state=1, shuffle=True)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1), binary=True, stop_words='english')\n",
        "vectorizer.fit(list(x_train) + list(x_test))\n",
        "\n",
        "x_train_vec = vectorizer.transform(x_train)\n",
        "x_test_vec = vectorizer.transform(x_test)\n",
        "print(x_train_vec.shape)\n",
        "print(x_test_vec.shape)\n",
        "\n",
        "# Linear SVM Classifier\n",
        "linear = svm.SVC(kernel='linear', C=1)\n",
        "linear.fit(x_train_vec, y_train)\n",
        "linear_pred = linear.predict(x_test_vec)\n",
        "score_linear = accuracy_score(linear_pred, y_test)\n",
        "print(\"Accuracy with Linear SVM: \", score_linear * 100, '%')\n",
        "\n",
        "# RBF SVM Classifier\n",
        "rbf = svm.SVC(kernel='rbf', gamma='scale', C=1)\n",
        "rbf.fit(x_train_vec, y_train)\n",
        "rbf_pred = rbf.predict(x_test_vec)\n",
        "score_rbf = accuracy_score(rbf_pred, y_test)\n",
        "print(\"Accuracy with RBF SVM: \", score_rbf * 100, '%')\n",
        "\n",
        "# Polynomial SVM Classifier\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1)\n",
        "poly.fit(x_train_vec, y_train)\n",
        "poly_pred = poly.predict(x_test_vec)\n",
        "score_poly = accuracy_score(poly_pred, y_test)\n",
        "print(\"Accuracy with Polynomial SVM: \", score_poly * 100, '%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, linear_pred, labels=linear.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=linear.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Linear SVM Confusion Matrix: \")\n",
        "print(confusion_matrix(y_test, linear_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, linear_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
